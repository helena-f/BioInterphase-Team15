{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfm+8i3TaSMJxRFVoESraR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8Q-pylyR-7q","executionInfo":{"status":"ok","timestamp":1728881156897,"user_tz":240,"elapsed":2245,"user":{"displayName":"Anushka Wani","userId":"07867696357221845994"}},"outputId":"0f96774e-13ba-421f-86be-041ffe26049d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","exists\n","exists\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","\n","habsosdata = '/content/drive/My Drive/Team15-Biointerphase/Dataset/(final)cleaned_habsos_data_v3.csv'\n","argosdata = '/content/drive/My Drive/Team15-Biointerphase/Dataset/ARGO_FloatR.csv'\n","\n","if os.path.exists(habsosdata):\n","    print(\"exists\")\n","else:\n","    print(\"file not found\")\n","\n","if os.path.exists(argosdata):\n","    print(\"exists\")\n","else:\n","    print(\"file not found\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the first dataset with default settings\n","habsos = pd.read_csv(habsosdata)\n","\n","# Define the column types for the second dataset to save memory\n","dtype = {\n","    'latitude': 'float32',\n","    'longitude': 'float32'\n","}\n","\n","# Load the second dataset in chunks\n","chunk_size = 50000  # You can adjust this size as necessary\n","chunks = []\n","\n","for chunk in pd.read_csv(argosdata, header=0, skiprows=[1], dtype=dtype, chunksize=chunk_size):\n","    # Rename columns as needed\n","    chunk.rename(columns={'time': 'SAMPLE_DATE'}, inplace=True)\n","    chunk.rename(columns={'latitude': 'LATITUDE'}, inplace=True)\n","    chunk.rename(columns={'longitude': 'LONGITUDE'}, inplace=True)\n","\n","    # Convert SAMPLE_DATE to datetime if it wasn't handled in dtype\n","    chunk['SAMPLE_DATE'] = pd.to_datetime(chunk['SAMPLE_DATE'])\n","\n","    # Append the chunk to the list\n","    chunks.append(chunk)\n","\n","# Concatenate all chunks into a single DataFrame\n","second_dataset = pd.concat(chunks, ignore_index=True)\n","\n","# Convert SAMPLE_DATE in habsos to naive datetime\n","habsos['SAMPLE_DATE'] = pd.to_datetime(habsos['SAMPLE_DATE']).dt.tz_localize(None)\n","\n","# Convert SAMPLE_DATE in second_dataset to naive datetime (if it has timezone)\n","second_dataset['SAMPLE_DATE'] = second_dataset['SAMPLE_DATE'].dt.tz_localize(None)\n","\n","# Merge the two datasets based on 'SAMPLE_DATE', 'latitude', and 'longitude'\n","merged_data = pd.merge(\n","    habsos,\n","    second_dataset,\n","    on=['SAMPLE_DATE', 'LATITUDE', 'LONGITUDE'],\n","    how='left',  # Use 'left' join to keep all rows from habsos\n","    suffixes=('_habsos', '_second')\n",")\n","\n","# Save the merged data to a new CSV file\n","merged_data.to_csv('merged_output.csv', index=False)\n","\n","print(\"Merging complete. The merged data is saved in 'merged_output.csv'.\")\n"],"metadata":{"id":"--99c21XVlqq"},"execution_count":null,"outputs":[]}]}